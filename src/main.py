import os
import json
import requests
import tweepy
import urllib.parse
from datetime import datetime, timedelta, timezone

# ================= é…ç½®åŒºåŸŸ =================
REPO_URL = "https://github.com/anonym-g/Attention"
TWITTER_USERNAME = "trailblaziger"

# è¯­è¨€é…ç½®
LANG_CONFIG = [
    {'code': 'en', 'project': 'en.wikipedia.org', 'name': 'English', 'flag': 'ğŸ‡ºğŸ‡¸',
     'header': 'English Wikipedia Top 10'},
    {'code': 'zh', 'project': 'zh.wikipedia.org', 'name': 'ä¸­æ–‡', 'flag': 'ğŸ‡¨ğŸ‡³', 'header': 'ä¸­æ–‡ç»´åŸºç™¾ç§‘æµè§ˆé‡ Top 10'},
    {'code': 'ja', 'project': 'ja.wikipedia.org', 'name': 'æ—¥æœ¬èª', 'flag': 'ğŸ‡¯ğŸ‡µ',
     'header': 'ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢é–²è¦§æ•° Top 10'},
    {'code': 'de', 'project': 'de.wikipedia.org', 'name': 'Deutsch', 'flag': 'ğŸ‡©ğŸ‡ª', 'header': 'Wikipedia Top 10 (DE)'},
    {'code': 'fr', 'project': 'fr.wikipedia.org', 'name': 'FranÃ§ais', 'flag': 'ğŸ‡«ğŸ‡·', 'header': 'Wikipedia Top 10 (FR)'},
    {'code': 'ru', 'project': 'ru.wikipedia.org', 'name': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹', 'flag': 'ğŸ‡·ğŸ‡º', 'header': 'Wikipedia Top 10 (RU)'},
    {'code': 'it', 'project': 'it.wikipedia.org', 'name': 'Italiano', 'flag': 'ğŸ‡®ğŸ‡¹', 'header': 'Wikipedia Top 10 (IT)'},
]

HEADERS = {
    'User-Agent': 'Attention-Bot/3.0 (https://github.com/anonym-g/Attention)'
}

# ================= è¿‡æ»¤é…ç½® =================

# 1. å‘½åç©ºé—´å‰ç¼€é»‘åå• (åŒ…å« EN, ZH, JA, DE, FR, RU, IT çš„å¸¸è§éæ¡ç›®ç©ºé—´)
IGNORE_PREFIXES = (
    # --- è‹±æ–‡/é€šç”¨ (APIæœ‰æ—¶è¿”å›é€šç”¨å‰ç¼€) ---
    'Special:', 'Wikipedia:', 'File:', 'Image:', 'Category:', 'Template:',
    'Help:', 'Portal:', 'Draft:', 'Talk:', 'User:', 'MediaWiki:', 'Book:',

    # --- ä¸­æ–‡ (ZH) ---
    'Special:', 'Wikipedia:', 'File:', 'Category:', 'Template:', 'Help:', 'Portal:', 'Draft:', 'Talk:', 'User:',
    'æ–‡ä»¶:', 'åˆ†ç±»:', 'æ¨¡ç‰ˆ:', 'æ¨¡æ¿:', 'å¸®åŠ©:', 'ä¼ é€é—¨:', 'è‰ç¨¿:', 'è®¨è®º:', 'ç”¨æˆ·:', 'è¯é¢˜:',

    # --- æ—¥è¯­ (JA) ---
    'ç‰¹åˆ¥:', 'Wikipedia:', 'ãƒ•ã‚¡ã‚¤ãƒ«:', 'Category:', 'Template:', 'Help:', 'Portal:', 'Draft:', 'Talk:', 'User:',
    'åˆ©ç”¨è€…:', 'ãƒãƒ¼ãƒˆ:', 'ç”»åƒ:',

    # --- å¾·è¯­ (DE) ---
    'Spezial:', 'Wikipedia:', 'Datei:', 'Kategorie:', 'Vorlage:', 'Hilfe:', 'Portal:', 'Diskussion:', 'Benutzer:',

    # --- æ³•è¯­ (FR) ---
    'SpÃ©cial:', 'WikipÃ©dia:', 'Fichier:', 'CatÃ©gorie:', 'ModÃ¨le:', 'Aide:', 'Portail:', 'Discussion:', 'Utilisateur:',

    # --- ä¿„è¯­ (RU) ---
    'Ğ¡Ğ»ÑƒĞ¶ĞµĞ±Ğ½Ğ°Ñ:', 'Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ñ:', 'Ğ¤Ğ°Ğ¹Ğ»:', 'ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ:', 'Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½:', 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞºĞ°:', 'ĞŸĞ¾Ñ€Ñ‚Ğ°Ğ»:', 'ĞĞ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ:', 'Ğ£Ñ‡Ğ°ÑÑ‚Ğ½Ğ¸Ğº:',

    # --- æ„å¤§åˆ©è¯­ (IT) ---
    'Speciale:', 'Wikipedia:', 'File:', 'Categoria:', 'Template:', 'Aiuto:', 'Portale:', 'Discussione:', 'Utente:',
)

# 2. ç²¾ç¡®åŒ¹é…é»‘åå• (ä¸»è¦æ˜¯å„å›½é¦–é¡µã€æœç´¢é¡µã€404ã€éšç§å£°æ˜ç­‰)
SPECIFIC_IGNORE_TERMS = [
    # --- é¦–é¡µ (Main Pages) ---
    'Main_Page',  # EN
    'Wikipedia:é¦–é¡µ', 'é¦–é¡µ',  # ZH
    'ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸',  # JA
    'Wikipedia:Hauptseite',  # DE
    'WikipÃ©dia:Accueil_principal',  # FR
    'Ğ—Ğ°Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ_ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°',  # RU
    'Pagina_principale',  # IT

    # --- æœç´¢é¡µ (Search) ---
    'Special:Search', 'Special:æœç´¢', 'Special:Recherche', 'Spezial:Suche',
    'Ğ¡Ğ»ÑƒĞ¶ĞµĞ±Ğ½Ğ°Ñ:ĞŸĞ¾Ğ¸ÑĞº', 'Speciale:Ricerca',

    # --- ç³»ç»Ÿ/é”™è¯¯é¡µ ---
    '-', '404.php', 'Nap', 'Undefined',

    # --- å…¶ä»–å¸¸è§å¹²æ‰°é¡¹ ---
    'Special:CreateAccount', 'Special:Watchlist', 'Special:RecentChanges',
    'Cookie_Statement', 'Privacy_policy',
    'Wikipedia:About', 'Wikipedia:General_disclaimer'
]

def get_date_str(date_obj):
    return date_obj.strftime("%Y-%m-%d")

def format_number(num):
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸ºåƒåˆ†ä½ï¼Œå¦‚ 1,234,567"""
    return f"{num:,}"

def get_top_articles(lang_code, date_obj):
    """è·å– Top 10 æ¡ç›®åŠå…¶æµè§ˆé‡"""
    year = date_obj.strftime("%Y")
    month = date_obj.strftime("%m")
    day = date_obj.strftime("%d")

    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{lang_code}.wikipedia/all-access/{year}/{month}/{day}"

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 404:
            return []
        response.raise_for_status()
        data = response.json()

        raw_articles = data.get('items', [])[0].get('articles', [])
        cleaned_data = []  # å­˜å‚¨å­—å…¸ {'title': ..., 'views': ...}

        for art in raw_articles:
            title = art['article']
            views = art['views']

            if title in SPECIFIC_IGNORE_TERMS:
                continue
            if title.startswith(IGNORE_PREFIXES):
                continue

            # å­˜å‚¨åŸå§‹æ ‡é¢˜(ç”¨äºé“¾æ¥)å’Œæµè§ˆé‡
            cleaned_data.append({'title': title, 'views': views})

            if len(cleaned_data) >= 10:
                break

        return cleaned_data
    except Exception as e:
        print(f"Error fetching {lang_code}: {e}")
        return []

def generate_link(project, articles_data, end_date_obj):
    """ç”Ÿæˆè¶‹åŠ¿å›¾é“¾æ¥"""
    if not articles_data:
        return None

    # æå–çº¯æ ‡é¢˜åˆ—è¡¨ç”¨äºç”Ÿæˆ URL
    titles = [item['title'] for item in articles_data]

    start_date = end_date_obj - timedelta(days=60)
    base_url = "https://pageviews.wmcloud.org/pageviews/"
    params = {
        "project": project,
        "platform": "all-access",
        "agent": "user",
        "redirects": "0",
        "start": start_date.strftime("%Y-%m-%d"),
        "end": end_date_obj.strftime("%Y-%m-%d"),
        "pages": "|".join(titles)
    }
    return f"{base_url}?{urllib.parse.urlencode(params, safe='|')}"

def construct_tweet(lang_config, date_str, articles_data, chart_link):
    """æ„å»ºç¬¦åˆè¦æ±‚çš„æ¨æ–‡å†…å®¹"""
    if not articles_data:
        return None

    # 1. æ ‡é¢˜è¡Œ
    header = f"{lang_config['flag']} {lang_config['header']} ({date_str})"

    # 2. æ„å»º Top 3 åˆ—è¡¨ (çº¯æ–‡æœ¬ï¼Œæ›¿æ¢ä¸‹åˆ’çº¿)
    top_lines = []
    for i, item in enumerate(articles_data[:3]):
        # æ ‡é¢˜å¤„ç†ï¼šå»ä¸‹åˆ’çº¿
        display_title = item['title'].replace('_', ' ')
        # æˆªæ–­å¤„ç†ï¼šé˜²æ­¢æ ‡é¢˜è¿‡é•¿åƒæ‰å­—ç¬¦æ•° (ä¿ç•™å‰20ä¸ªå­—ç¬¦ + ...)
        if len(display_title) > 25:
            display_title = display_title[:24] + "â€¦"

        views_str = format_number(item['views'])
        top_lines.append(f"{i + 1}. {display_title}: {views_str}")

    top_content = "\n".join(top_lines)

    # 3. ç»„åˆæ¨æ–‡
    # è¯´æ˜ï¼šTwitteré“¾æ¥å 23å­—ç¬¦ã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªé“¾æ¥ï¼Œå…±å 46å­—ç¬¦ã€‚
    # å‰©ä½™å¯ç”¨ï¼š280 - 46 = 234ã€‚
    # Header + Top3 + CTA + Label éœ€å°äº 234ã€‚

    tweet_text = (
        f"{header}\n\n"
        f"{top_content}\n\n"
        f"ğŸ“Š Visualization & Full List:\n"  # CTA è¯´æ˜
        f"{chart_link}\n\n"
        f"ğŸ”— Project: {REPO_URL}"
    )

    return tweet_text

def save_data(date_str, data):
    """
    å°†æ•°æ®ä¿å­˜åˆ° data/ æ–‡ä»¶å¤¹
    """
    # è·å– main.py æ‰€åœ¨ç›®å½• (src/) çš„çˆ¶ç›®å½• (æ ¹ç›®å½•)
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    dir_path = os.path.join(base_dir, "data")
    file_path = os.path.join(dir_path, f"{date_str}.json")

    try:
        os.makedirs(dir_path, exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"-> Data saved to: {file_path}")
    except Exception as e:
        print(f"Error saving data: {e}")

def update_readme(date_str, tweet_id):
    """
    æ›´æ–° README.mdï¼Œåœ¨ Tweet List åŒºåŸŸè¿½åŠ æ–°çš„æ¨æ–‡é“¾æ¥
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    readme_path = os.path.join(base_dir, "README.md")

    link = f"https://x.com/{TWITTER_USERNAME}/status/{tweet_id}"
    line_to_add = f"#### {date_str}: {link}"

    try:
        if not os.path.exists(readme_path):
            print("README.md not found, skipping update.")
            return

        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # é˜²æ­¢é‡å¤æ·»åŠ 
        if link in content:
            print(f"Link already in README: {link}")
            return

        with open(readme_path, 'a', encoding='utf-8') as f:
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œå…ˆæ·»åŠ æ ‡é¢˜
            if "## Tweet List" not in content:
                f.write("\n\n## Tweet List\n")

            # ç¡®ä¿æ–°è¡Œå‰æœ‰æ¢è¡Œç¬¦ (å¦‚æœæ–‡ä»¶æœ«å°¾ä¸æ˜¯æ¢è¡Œ)
            elif not content.endswith('\n'):
                f.write("\n")

            f.write(f"{line_to_add}\n")
            print(f"-> Added to README: {line_to_add}")

    except Exception as e:
        print(f"Error updating README: {e}")

def get_twitter_client():
    api_key = os.environ.get("TWITTER_API_KEY")
    api_secret = os.environ.get("TWITTER_API_SECRET")
    access_token = os.environ.get("TWITTER_ACCESS_TOKEN")
    access_token_secret = os.environ.get("TWITTER_ACCESS_TOKEN_SECRET")

    if not all([api_key, api_secret, access_token, access_token_secret]):
        print("Twitter secrets missing.")
        return None

    return tweepy.Client(
        consumer_key=api_key,
        consumer_secret=api_secret,
        access_token=access_token,
        access_token_secret=access_token_secret
    )

def main():
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = get_date_str(yesterday)
    print(f"--- Report Date: {date_str} ---")

    report_data = {"date": date_str, "results": []}
    client = get_twitter_client()

    # ç»´æŠ¤ä¸€ä¸ªæœ‰æ•ˆID
    # å¦‚æœä¸­é—´æŸæ¡å¤±è´¥ï¼Œè¿™ä¸ªIDä¸æ›´æ–°ï¼Œä¸‹ä¸€æ¡ä¼šè‡ªåŠ¨å›å¤ä¸Šä¸€ä¸ªæˆåŠŸçš„æ¨æ–‡ï¼Œä¿è¯Threadä¸æ–­
    last_successful_id = None

    for lang in LANG_CONFIG:
        print(f"Processing {lang['code']}...")

        articles_data = get_top_articles(lang['code'], yesterday)
        if not articles_data:
            print(f"No data for {lang['code']}, skipping.")
            continue

        link = generate_link(lang['project'], articles_data, yesterday)

        report_data["results"].append({
            "lang": lang['code'],
            "data": articles_data,
            "link": link
        })

        tweet_text = construct_tweet(lang, date_str, articles_data, link)

        print(f"\n[Preview {lang['code']}] Length: {len(tweet_text)}")
        print(tweet_text)
        print("-" * 30)

        if client:
            try:
                # å°è¯•å‘é€
                if last_successful_id:
                    resp = client.create_tweet(text=tweet_text, in_reply_to_tweet_id=last_successful_id)
                else:
                    resp = client.create_tweet(text=tweet_text)

                # æµ‹è¯•æœ‰æ•ˆï¼Œæ›´æ–°ID
                last_successful_id = resp.data['id']
                print(f"Posted {lang['code']} successfully. ID: {last_successful_id}")

                # å¦‚æœæ˜¯è‹±æ–‡ç‰ˆï¼Œå°†é“¾æ¥å†™å…¥ README.md
                if lang['code'] == 'en':
                    update_readme(date_str, last_successful_id)

            except Exception as e:
                # å¤±è´¥åˆ™æ‰“å°é”™è¯¯ï¼Œlast_successful_id ä¿æŒä¸å˜
                print(f"Failed to post {lang['code']}: {e}")

    save_data(date_str, report_data)
    print("\nAll done.")

if __name__ == "__main__":
    main()
