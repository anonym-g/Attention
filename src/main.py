import os
import json
import time
import requests
import tweepy
import urllib.parse
from datetime import datetime, timedelta, timezone
from playwright.sync_api import sync_playwright, ViewportSize, Browser, FloatRect

# ================= é…ç½®åŒºåŸŸ =================
REPO_URL = "https://github.com/anonym-g/Attention"
TWITTER_USERNAME = "trailblaziger"

# è¯­è¨€é…ç½®
LANG_CONFIG = [
    {'code': 'en', 'project': 'en.wikipedia.org', 'name': 'English', 'flag': 'ğŸ‡ºğŸ‡¸',
     'header': 'English Wikipedia Top 10'},
    {'code': 'zh', 'project': 'zh.wikipedia.org', 'name': 'ä¸­æ–‡', 'flag': 'ğŸ‡¨ğŸ‡³', 'header': 'ä¸­æ–‡ç»´åŸºç™¾ç§‘æµè§ˆé‡ Top 10'},
    {'code': 'ja', 'project': 'ja.wikipedia.org', 'name': 'æ—¥æœ¬èª', 'flag': 'ğŸ‡¯ğŸ‡µ',
     'header': 'ã‚¦ã‚£ã‚­ãƒšãƒ‡ã‚£ã‚¢é–²è¦§æ•° Top 10'},
    {'code': 'de', 'project': 'de.wikipedia.org', 'name': 'Deutsch', 'flag': 'ğŸ‡©ğŸ‡ª', 'header': 'Wikipedia Top 10 (DE)'},
    {'code': 'fr', 'project': 'fr.wikipedia.org', 'name': 'FranÃ§ais', 'flag': 'ğŸ‡«ğŸ‡·', 'header': 'Wikipedia Top 10 (FR)'},
    {'code': 'ru', 'project': 'ru.wikipedia.org', 'name': 'Ğ ÑƒÑÑĞºĞ¸Ğ¹', 'flag': 'ğŸ‡·ğŸ‡º', 'header': 'Wikipedia Top 10 (RU)'},
    {'code': 'it', 'project': 'it.wikipedia.org', 'name': 'Italiano', 'flag': 'ğŸ‡®ğŸ‡¹', 'header': 'Wikipedia Top 10 (IT)'},
]

HEADERS = {
    'User-Agent': 'Attention-Bot/3.0 (https://github.com/anonym-g/Attention)'
}

# ================= è¿‡æ»¤é…ç½® =================

# 1. å‘½åç©ºé—´å‰ç¼€é»‘åå• (åŒ…å« EN, ZH, JA, DE, FR, RU, IT çš„å¸¸è§éæ¡ç›®ç©ºé—´)
IGNORE_PREFIXES = (
    # --- è‹±æ–‡/é€šç”¨ (APIæœ‰æ—¶è¿”å›é€šç”¨å‰ç¼€) ---
    'Special:', 'Wikipedia:', 'File:', 'Image:', 'Category:', 'Template:',
    'Help:', 'Portal:', 'Draft:', 'Talk:', 'User:', 'MediaWiki:', 'Book:',

    # --- ä¸­æ–‡ (ZH) ---
    'æ–‡ä»¶:', 'åˆ†ç±»:', 'æ¨¡ç‰ˆ:', 'æ¨¡æ¿:', 'å¸®åŠ©:', 'ä¼ é€é—¨:', 'è‰ç¨¿:', 'è®¨è®º:', 'ç”¨æˆ·:', 'è¯é¢˜:',

    # --- æ—¥è¯­ (JA) ---
    'ç‰¹åˆ¥:', 'ãƒ•ã‚¡ã‚¤ãƒ«:', 'åˆ©ç”¨è€…:', 'ãƒãƒ¼ãƒˆ:', 'ç”»åƒ:',

    # --- å¾·è¯­ (DE) ---
    'Spezial:', 'Datei:', 'Kategorie:', 'Vorlage:', 'Hilfe:', 'Diskussion:', 'Benutzer:',

    # --- æ³•è¯­ (FR) ---
    'SpÃ©cial:', 'WikipÃ©dia:', 'Fichier:', 'CatÃ©gorie:', 'ModÃ¨le:', 'Aide:', 'Portail:', 'Discussion:', 'Utilisateur:',

    # --- ä¿„è¯­ (RU) ---
    'Ğ¡Ğ»ÑƒĞ¶ĞµĞ±Ğ½Ğ°Ñ:', 'Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ñ:', 'Ğ¤Ğ°Ğ¹Ğ»:', 'ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ:', 'Ğ¨Ğ°Ğ±Ğ»Ğ¾Ğ½:', 'Ğ¡Ğ¿Ñ€Ğ°Ğ²ĞºĞ°:', 'ĞŸĞ¾Ñ€Ñ‚Ğ°Ğ»:', 'ĞĞ±ÑÑƒĞ¶Ğ´ĞµĞ½Ğ¸Ğµ:', 'Ğ£Ñ‡Ğ°ÑÑ‚Ğ½Ğ¸Ğº:',

    # --- æ„å¤§åˆ©è¯­ (IT) ---
    'Speciale:', 'Categoria:', 'Aiuto:', 'Portale:', 'Discussione:', 'Utente:',
)

# 2. ç²¾ç¡®åŒ¹é…é»‘åå• (ä¸»è¦æ˜¯å„å›½é¦–é¡µã€æœç´¢é¡µã€404ã€éšç§å£°æ˜ç­‰)
SPECIFIC_IGNORE_TERMS = [
    # --- é¦–é¡µ (Main Pages) ---
    'Main_Page',  # EN
    'Wikipedia:é¦–é¡µ', 'é¦–é¡µ',  # ZH
    'ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸',  # JA
    'Wikipedia:Hauptseite',  # DE
    'WikipÃ©dia:Accueil_principal',  # FR
    'Ğ—Ğ°Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ_ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ°',  # RU
    'Pagina_principale',  # IT

    # --- æœç´¢é¡µ (Search) ---
    'Special:Search', 'Special:æœç´¢', 'Special:Recherche', 'Spezial:Suche',
    'Ğ¡Ğ»ÑƒĞ¶ĞµĞ±Ğ½Ğ°Ñ:ĞŸĞ¾Ğ¸ÑĞº', 'Speciale:Ricerca',

    # --- ç³»ç»Ÿ/é”™è¯¯é¡µ ---
    '-', '404.php', 'Nap', 'Undefined',

    # --- å…¶ä»–å¸¸è§å¹²æ‰°é¡¹ ---
    'Special:CreateAccount', 'Special:Watchlist', 'Special:RecentChanges',
    'Cookie_Statement', 'Privacy_policy',
    'Wikipedia:About', 'Wikipedia:General_disclaimer'
]

def get_date_str(date_obj):
    return date_obj.strftime("%Y-%m-%d")

def format_number(num):
    """å°†æ•°å­—æ ¼å¼åŒ–ä¸ºåƒåˆ†ä½ï¼Œå¦‚ 1,234,567"""
    return f"{num:,}"

def get_top_articles(lang_code, date_obj):
    """è·å– Top 10 æ¡ç›®åŠå…¶æµè§ˆé‡"""
    year = date_obj.strftime("%Y")
    month = date_obj.strftime("%m")
    day = date_obj.strftime("%d")

    url = f"https://wikimedia.org/api/rest_v1/metrics/pageviews/top/{lang_code}.wikipedia/all-access/{year}/{month}/{day}"

    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 404:
            return []
        response.raise_for_status()
        data = response.json()

        raw_articles = data.get('items', [])[0].get('articles', [])
        cleaned_data = []  # å­˜å‚¨å­—å…¸ {'title': ..., 'views': ...}

        for art in raw_articles:
            title = art['article']
            views = art['views']

            if title in SPECIFIC_IGNORE_TERMS:
                continue
            if title.startswith(IGNORE_PREFIXES):
                continue

            # å­˜å‚¨åŸå§‹æ ‡é¢˜(ç”¨äºé“¾æ¥)å’Œæµè§ˆé‡
            cleaned_data.append({'title': title, 'views': views})

            if len(cleaned_data) >= 10:
                break

        return cleaned_data
    except Exception as e:
        print(f"Error fetching {lang_code}: {e}")
        return []

def generate_link(project, articles_data, end_date_obj):
    """ç”Ÿæˆè¶‹åŠ¿å›¾é“¾æ¥"""
    if not articles_data:
        return None

    # æå–çº¯æ ‡é¢˜åˆ—è¡¨ç”¨äºç”Ÿæˆ URL
    titles = [item['title'] for item in articles_data]

    start_date = end_date_obj - timedelta(days=60)
    base_url = "https://pageviews.wmcloud.org/pageviews/"
    params = {
        "project": project,
        "platform": "all-access",
        "agent": "user",
        "redirects": "0",
        "start": start_date.strftime("%Y-%m-%d"),
        "end": end_date_obj.strftime("%Y-%m-%d"),
        "pages": "|".join(titles)
    }
    return f"{base_url}?{urllib.parse.urlencode(params, safe='|')}"

def construct_tweet(lang_config, date_str, articles_data, chart_link):
    """æ„å»ºç¬¦åˆè¦æ±‚çš„æ¨æ–‡å†…å®¹"""
    if not articles_data:
        return None

    # 1. æ ‡é¢˜è¡Œ
    header = f"{lang_config['flag']} {lang_config['header']} ({date_str})"

    # 2. æ„å»º Top 3 åˆ—è¡¨ (çº¯æ–‡æœ¬ï¼Œæ›¿æ¢ä¸‹åˆ’çº¿)
    top_lines = []
    for i, item in enumerate(articles_data[:3]):
        # æ ‡é¢˜å¤„ç†ï¼šå»ä¸‹åˆ’çº¿
        display_title = item['title'].replace('_', ' ')
        # æˆªæ–­å¤„ç†ï¼šé˜²æ­¢æ ‡é¢˜è¿‡é•¿åƒæ‰å­—ç¬¦æ•° (ä¿ç•™å‰20ä¸ªå­—ç¬¦ + ...)
        if len(display_title) > 25:
            display_title = display_title[:24] + "â€¦"

        views_str = format_number(item['views'])
        top_lines.append(f"{i + 1}. {display_title}: {views_str}")

    top_content = "\n".join(top_lines)

    # 3. ç»„åˆæ¨æ–‡
    # è¯´æ˜ï¼šTwitteré“¾æ¥å 23å­—ç¬¦ã€‚è¿™é‡Œæœ‰ä¸¤ä¸ªé“¾æ¥ï¼Œå…±å 46å­—ç¬¦ã€‚
    # å‰©ä½™å¯ç”¨ï¼š280 - 46 = 234ã€‚
    # Header + Top3 + CTA + Label éœ€å°äº 234ã€‚

    tweet_text = (
        f"{header}\n\n"
        f"{top_content}\n\n"
        f"ğŸ“Š Visualization & Full List:\n"  # CTA è¯´æ˜
        f"{chart_link}\n\n"
        f"ğŸ”— Project: {REPO_URL}"
    )

    return tweet_text

def save_data(date_str, data):
    """
    å°†æ•°æ®ä¿å­˜åˆ° data/ æ–‡ä»¶å¤¹
    """
    # è·å– main.py æ‰€åœ¨ç›®å½• (src/) çš„çˆ¶ç›®å½• (æ ¹ç›®å½•)
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    dir_path = os.path.join(base_dir, "data")
    file_path = os.path.join(dir_path, f"{date_str}.json")

    try:
        os.makedirs(dir_path, exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"-> Data saved to: {file_path}")
    except Exception as e:
        print(f"Error saving data: {e}")

def update_readme(date_str, tweet_id):
    """
    æ›´æ–° README.mdï¼Œåœ¨ Tweet List åŒºåŸŸè¿½åŠ æ–°çš„æ¨æ–‡é“¾æ¥
    """
    # è·å–é¡¹ç›®æ ¹ç›®å½•
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    readme_path = os.path.join(base_dir, "README.md")

    link = f"https://x.com/{TWITTER_USERNAME}/status/{tweet_id}"
    line_to_add = f"#### {date_str}: {link}"

    try:
        if not os.path.exists(readme_path):
            print("README.md not found, skipping update.")
            return

        with open(readme_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # é˜²æ­¢é‡å¤æ·»åŠ 
        if link in content:
            print(f"Link already in README: {link}")
            return

        with open(readme_path, 'a', encoding='utf-8') as f:
            # å¦‚æœæ²¡æœ‰æ ‡é¢˜ï¼Œå…ˆæ·»åŠ æ ‡é¢˜
            if "## Tweet List" not in content:
                f.write("\n\n## Tweet List\n")

            # ç¡®ä¿æ–°è¡Œå‰æœ‰æ¢è¡Œç¬¦ (å¦‚æœæ–‡ä»¶æœ«å°¾ä¸æ˜¯æ¢è¡Œ)
            elif not content.endswith('\n'):
                f.write("\n")

            f.write(f"{line_to_add}\n")
            print(f"-> Added to README: {line_to_add}")

    except Exception as e:
        print(f"Error updating README: {e}")

def ensure_picture_dir(date_str, lang_code):
    """åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•"""
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    path = os.path.join(base_dir, "pictures", date_str, lang_code)
    os.makedirs(path, exist_ok=True)
    return path

def capture_screenshots(url, save_dir):
    """ä½¿ç”¨ Playwright æˆªå– Logarithmic Line Chart å’Œ Pie Chart"""
    if not url:
        return []

    line_path = os.path.join(save_dir, "line.png")
    pie_path = os.path.join(save_dir, "pie.png")

    if os.path.exists(line_path) and os.path.exists(pie_path):
        print(f"Images already exist in {save_dir}, skipping.")
        return [line_path, pie_path]

    images = []

    try:
        with sync_playwright() as p:
            print("Launching browser...")
            browser: Browser = p.chromium.launch(headless=True)

            viewport_size: ViewportSize = {"width": 2560, "height": 1440}
            page = browser.new_page(viewport=viewport_size)

            print(f"Navigating to: {url}")
            page.goto(url, wait_until='networkidle', timeout=90000)

            try:
                page.wait_for_selector("canvas", state="visible", timeout=30000)
                time.sleep(5)
            except Exception as e:
                print(f"Warning: Canvas not detected or slow loading: {e}")

            def take_smart_screenshot(file_path):
                """æ™ºèƒ½æˆªå›¾"""
                try:
                    canvas = page.locator("canvas").first
                    box = canvas.bounding_box()
                    if box:
                        bottom_y = box['y'] + box['height'] + 180
                        clip_rect: FloatRect = {
                            'x': 0.0,
                            'y': 0.0,
                            'width': 2560.0,
                            'height': float(bottom_y)
                        }
                        page.screenshot(path=file_path, clip=clip_rect)
                        print(f"Captured (Smart): {file_path}")
                    else:
                        raise Exception("Canvas bounding box is None")
                except Exception as err:
                    print(f"Smart screenshot failed ({err}), falling back to viewport screenshot.")
                    page.screenshot(path=file_path)
                    print(f"Captured (Fallback): {file_path}")

            # 1. Logarithmic Line Chart
            try:
                # ä½¿ç”¨ exact=True é¿å…åŒ¹é…åˆ° "Automatically use logarithmic"
                # æˆ–è€…ç›´æ¥å®šä½ class
                log_label = page.locator(".logarithmic-scale").first
                if log_label.is_visible():
                    log_label.click()
                    time.sleep(3)
                else:
                    # å¤‡é€‰ï¼šç²¾ç¡®æ–‡æœ¬åŒ¹é…
                    page.get_by_text("Logarithmic scale", exact=True).click()
                    time.sleep(3)
            except Exception as e:
                print(f"Error toggling Logarithmic: {e}")

            take_smart_screenshot(line_path)
            images.append(line_path)

            # 2. Pie Chart
            try:
                # ä½¿ç”¨ CSS ç±»å®šä½æŒ‰é’®ï¼Œé¿å…åŒ¹é…åˆ°æ¨¡æ€æ¡†æ ‡é¢˜ "Chart types"
                chart_btn = page.locator(".btn-chart-type").first
                if chart_btn.is_visible():
                    chart_btn.click()
                    time.sleep(1)

                    # åœ¨ä¸‹æ‹‰èœå•ä¸­ç‚¹å‡» "Pie"
                    pie_option = page.get_by_text("Pie", exact=True)
                    if pie_option.is_visible():
                        pie_option.click()
                        time.sleep(3)
                    else:
                        print("Pie option not visible.")
                else:
                    print("Chart type button not found.")
            except Exception as e:
                print(f"Error toggling Pie chart: {e}")

            take_smart_screenshot(pie_path)
            images.append(pie_path)

            browser.close()

    except Exception as e:
        print(f"Playwright critical error: {e}")

    return images

def get_twitter_auth_v1():
    """è·å– v1.1 API (ç”¨äºä¸Šä¼ åª’ä½“)"""
    api_key = os.environ.get("TWITTER_API_KEY")
    api_secret = os.environ.get("TWITTER_API_SECRET")
    access_token = os.environ.get("TWITTER_ACCESS_TOKEN")
    access_token_secret = os.environ.get("TWITTER_ACCESS_TOKEN_SECRET")

    if not all([api_key, api_secret, access_token, access_token_secret]):
        return None

    auth = tweepy.OAuth1UserHandler(api_key, api_secret, access_token, access_token_secret)
    return tweepy.API(auth)

def get_twitter_client_v2():
    api_key = os.environ.get("TWITTER_API_KEY")
    api_secret = os.environ.get("TWITTER_API_SECRET")
    access_token = os.environ.get("TWITTER_ACCESS_TOKEN")
    access_token_secret = os.environ.get("TWITTER_ACCESS_TOKEN_SECRET")

    if not all([api_key, api_secret, access_token, access_token_secret]):
        print("Twitter secrets missing.")
        return None

    return tweepy.Client(
        consumer_key=api_key,
        consumer_secret=api_secret,
        access_token=access_token,
        access_token_secret=access_token_secret
    )

def main():
    yesterday = datetime.now(timezone.utc) - timedelta(days=1)
    date_str = get_date_str(yesterday)
    print(f"--- Report Date: {date_str} ---")

    report_data = {"date": date_str, "results": []}

    # 1. å‡†å¤‡é˜¶æ®µï¼šæŠ“å–æ•°æ®ï¼Œç”Ÿæˆé“¾æ¥ã€æˆªå›¾ï¼Œæ„å»ºæ–‡æœ¬
    tweet_queue = []

    print(">>> Phase 1: Preparing content (Data & Screenshots)...")
    for lang in LANG_CONFIG:
        print(f"\nProcessing {lang['code']}...")

        articles_data = get_top_articles(lang['code'], yesterday)
        if not articles_data:
            print(f"No data for {lang['code']}, skipping.")
            continue

        link = generate_link(lang['project'], articles_data, yesterday)

        # --- æˆªå›¾ ---
        print("Taking screenshots...")
        pic_dir = ensure_picture_dir(date_str, lang['code'])
        image_paths = capture_screenshots(link, pic_dir)

        # --- æ„å»ºæ¨æ–‡æ–‡æœ¬ ---
        tweet_text = construct_tweet(lang, date_str, articles_data, link)
        print(f"[Content Preview] {tweet_text[:50]}...")

        # å­˜å…¥æ•°æ®æŠ¥å‘Š
        report_data["results"].append({
            "lang": lang['code'],
            "data": articles_data,
            "link": link,
            "images": image_paths
        })

        # å­˜å…¥å‘æ¨é˜Ÿåˆ—
        tweet_queue.append({
            "lang_code": lang['code'],
            "text": tweet_text,
            "images": image_paths
        })

    # 2. å‘é€é˜¶æ®µï¼šæ‰¹é‡ä¸Šä¼ å›¾ç‰‡å¹¶å‘é€ Thread
    print("\n>>> Phase 2: Posting Tweets...")

    client_v2 = get_twitter_client_v2()
    api_v1 = get_twitter_auth_v1()
    last_successful_id = None

    if client_v2 and api_v1:
        for item in tweet_queue:
            lang_code = item['lang_code']
            text = item['text']
            images = item['images']

            try:
                media_ids = []
                # ä¸Šä¼ å›¾ç‰‡
                for img_path in images:
                    if os.path.exists(img_path):
                        print(f"[{lang_code}] Uploading {img_path}...")
                        media = api_v1.media_upload(filename=img_path)
                        media_ids.append(media.media_id)

                # å‘æ¨
                print(f"[{lang_code}] Sending tweet...")
                if last_successful_id:
                    resp = client_v2.create_tweet(
                        text=text,
                        media_ids=media_ids if media_ids else None,
                        in_reply_to_tweet_id=last_successful_id
                    )
                else:
                    resp = client_v2.create_tweet(
                        text=text,
                        media_ids=media_ids if media_ids else None
                    )

                # æµ‹è¯•æœ‰æ•ˆï¼Œæ›´æ–°ID
                last_successful_id = resp.data['id']
                print(f"[{lang_code}] Posted successfully. ID: {last_successful_id}")

                if lang_code == 'en':
                    update_readme(date_str, last_successful_id)

                # ç¨å¾®ç­‰å¾…é¿å…é€Ÿç‡é™åˆ¶
                time.sleep(2)

            except Exception as e:
                print(f"[{lang_code}] Failed to post: {e}")
    else:
        print("Twitter credentials missing, skipping post phase.")

    # 3. ä¿å­˜æ•°æ®
    save_data(date_str, report_data)
    print("\nAll done.")

if __name__ == "__main__":
    main()
